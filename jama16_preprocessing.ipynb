{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jama16-preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pichetzh/jama16-retina-replication/blob/master/jama16_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "_uuid": "34b6997bb115a11f47a7f54ce9d9052791b2e707",
        "_cell_guid": "c67e806c-e0e6-415b-8cf0-ed92eba5ed37",
        "id": "YS7ONzPpcW-8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "This notebook performs data preprocessing for jama16-retina-replication github:\n",
        "https://github.com/mikevoets/jama16-retina-replication\n",
        "\n",
        "The goal is to make a nice retinopathy model by using a pretrained inception v3 as a base and retraining some modified final layers with attention\n",
        "\n",
        "This can be massively improved with \n",
        "* high-resolution images\n",
        "* better data sampling\n",
        "* ensuring there is no leaking between training and validation sets, ```sample(replace = True)``` is real dangerous\n",
        "* better target variable (age) normalization\n",
        "* pretrained models\n",
        "* attention/related techniques to focus on areas"
      ]
    },
    {
      "metadata": {
        "id": "_-TLcRmheA9S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Clone  GIT repository\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "lkOfN5IYE7-Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initialize only once\n",
        "GIT_USERNAME = \"pichetzh\" \n",
        "GIT_TOKEN = \"68594bcc127e64863a341dbdcbd50ef55c8774e4\" \n",
        "GIT_REPOSITORY = \"jama16-retina-replication\" # by forking\n",
        "\n",
        "GIT_PATH = \"/content/_github\" # this folder must be empty at initialization\n",
        "!git clone --recursive https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git \"{GIT_PATH}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "er94MRGvH4QP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the create_tfrecords repository. \n",
        "# This tool will convert the data sets into TFRecord files\n",
        "%%bash\n",
        "cd /content/_github\n",
        "git submodule update --init --recursive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oaS6e-OAc_U1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "89SBKVX-c8PI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "\n",
        "ROOT = \"/content/_gdrive\"\n",
        "drive.mount(ROOT, force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pyj4ZwhCebFa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# extract master file\n",
        "\n",
        "import os\n",
        "from os.path import join\n",
        "import zipfile\n",
        "\n",
        "COLAB_PATH = '/content/_gdrive/My Drive/_colab'\n",
        "master_folder = 'master-inceptionv3-for-retinopathy'\n",
        "MASTER_PATH = join('/content', master_folder)\n",
        "\n",
        "path_to_zip_file = join(COLAB_PATH, 'projects', \n",
        "                        'diabetic-retinopathy', master_folder+'.zip')  \n",
        "directory_to_extract_to = '/content'\n",
        "zip_ref = zipfile.ZipFile(path_to_zip_file, 'r')\n",
        "zip_ref.extractall(directory_to_extract_to)\n",
        "zip_ref.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BZtsU-6Z5v5r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# extract data file\n",
        "\n",
        "import os\n",
        "from os.path import join\n",
        "import zipfile\n",
        "\n",
        "COLAB_PATH = '/content/_gdrive/My Drive/_colab'\n",
        "master_folder = 'master-inceptionv3-for-retinopathy'\n",
        "MASTER_PATH = join('/content', master_folder)\n",
        "\n",
        "\n",
        "upload_zip = 'eyepacs-52478-merged.zip'\n",
        "path_to_zip_file = join(COLAB_PATH, 'data', \n",
        "                        'diabetic_retinopathy', upload_zip)\n",
        "!mkdir -p /content/data\n",
        "directory_to_extract_to = '/content/data'\n",
        "zip_ref = zipfile.ZipFile(path_to_zip_file, 'r')\n",
        "zip_ref.extractall(directory_to_extract_to)\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JVojhhN15H1l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# extract models file\n",
        "path_to_zip_file = join(COLAB_PATH, 'models', 'keras-pretrained-models.zip')\n",
        "!mkdir -p /content/models\n",
        "directory_to_extract_to = '/content/models'\n",
        "zip_ref = zipfile.ZipFile(path_to_zip_file, 'r')\n",
        "zip_ref.extractall(directory_to_extract_to)\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iuwHUSx1Jlz7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# copy the weights and configurations for the pre-trained models\n",
        "%%bash\n",
        "#mkdir -p ~/.keras\n",
        "mkdir -p ~/.keras/models\n",
        "cd '/content/models/keras-pretrained-models'\n",
        "cp *notop.* ~/.keras/models/\n",
        "cp 'imagenet_class_index.json' ~/.keras/models/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B2ZOc6kOjUVa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -la ~/.keras/models/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VyQv62WBoAza",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Set Kaggle API Access"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_Kvuv5jldWE5"
      },
      "cell_type": "markdown",
      "source": [
        "# Load TrainLabels.csv"
      ]
    },
    {
      "metadata": {
        "_uuid": "725d378daf5f836d4885d67240fc7955f113309d",
        "_cell_guid": "c3cc4285-bfa4-4612-ac5f-13d10678c09a",
        "trusted": true,
        "id": "XPN8cY3YcW_C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt # showing and rendering figures\n",
        "# io related\n",
        "from skimage.io import imread\n",
        "import os\n",
        "from glob import glob\n",
        "# not needed in Kaggle, but required in Jupyter\n",
        "%matplotlib inline "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6RO2e5kfl_hG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# temporary fix for labels\n",
        "%%bash\n",
        "cd '/content/_gdrive/My Drive/_colab/data/diabetic_retinopathy/'\n",
        "ls -la\n",
        "mkdir -p /content/data/eyepacs-52478/eyepacs-labels\n",
        "cp eyepacs-labels/* /content/data/eyepacs-52478/eyepacs-labels/\n",
        "ls -la /content/data/eyepacs-52478/"
      ]
    },
    {
      "metadata": {
        "id": "sbdQX1t0Efs_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "a = pd.read_csv('/content/data/'+upload_folder+'/eyepacs-labels/trainLabels.csv')\n",
        "b = pd.read_csv('/content/data/'+upload_folder+'/eyepacs-labels/testLabels.csv')\n",
        "\n",
        "merged = pd.concat([a,b])\n",
        "merged.to_csv('/content/data/eyepacs-52478/eyepacs-labels/allLabels.csv', index=True)\n"
      ]
    },
    {
      "metadata": {
        "id": "95WLyXBjf-6r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "upload_folder = upload_zip.replace('.zip','')\n",
        "base_image_dir = '/content/data/'+upload_folder+'/eyepacs-train/'\n",
        "base_label_dir = '/content/data/'+upload_folder+'/eyepacs-labels/trainLabels.csv'\n",
        "\n",
        "retina_df = pd.read_csv(base_label_dir)\n",
        "retina_df.head()\n",
        "len(retina_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sQ8iAKfM2sDr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# merge all image files under /bin2 directory\n",
        "%%bash\n",
        "cd /content/data/eyepacs-52478/bin2\n",
        "find . -type f -print0 | xargs -0 -I file mv --backup=numbered file .\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y4cEvZ0k4XHw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path, dirs, files = next(os.walk(\"/content/data/eyepacs-52478/bin2\"))\n",
        "file_count = len(files)\n",
        "file_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_j7KJm4j4Wd9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/data/eyepacs-52478/bin2\n",
        "find . -type f -print0 | xargs -0 -I file mv --backup=numbered file ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hZ1X4Edw4Vno",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zErWpEOi3HXx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'/content/data/eyepacs-52478/bin2/10031_left.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DCZFvkBEyXGD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# need to combine {0,1} into train folder\n",
        "\n",
        "import re\n",
        "file_names = os.listdir(base_image_dir)\n",
        "print(len(file_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VjYlMdog0oVv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "file_names = [re.sub('.jpeg','', f) for f in file_names]\n",
        "print(len(file_names))\n",
        "file_names[:5]\n",
        "\n",
        "#retina_df = retina_df.loc[retina_df['image'].isin(file_names)]\n",
        "\n",
        "print(len(retina_df))\n",
        "print(len(file_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uZfdVOTfoahK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "upload_zip = 'eyepacs-52478.zip'\n",
        "\n",
        "upload_folder = upload_zip.replace('.zip','')\n",
        "base_image_dir = '/content/data/'+upload_folder+'/bin2/'\n",
        "base_label_dir = '/content/data/'+upload_folder+'/eyepacs-labels/allLabels.csv'\n",
        "\n",
        "retina_df = pd.read_csv(base_label_dir)\n",
        "retina_df.head(5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K0UknePnjR51",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data_path = '/content/data/eyepacs-52478-merged'\n",
        "base_image_dir = data_path +'/bin2/'\n",
        "base_label_dir = data_path +'/eyepacs-labels/allLabels.csv'\n",
        "\n",
        "retina_df = pd.read_csv(base_label_dir)\n",
        "print(len(retina_df))\n",
        "\n",
        "retina_df['PatientId'] = retina_df['image'].map(lambda x: x.split('_')[0])\n",
        "ext = '.jpg' if upload_folder == 'eyepacs-52478' else '.jpeg'\n",
        "retina_df['path'] = retina_df['image'].map(lambda x: base_image_dir + x + ext)\n",
        "retina_df['exists'] = retina_df['path'].map(os.path.exists)\n",
        "print(retina_df['exists'].sum(), 'images found of', retina_df.shape[0], 'total')\n",
        "retina_df['eye'] = retina_df['image'].map(lambda x: 1 if x.split('_')[-1]=='left' else 0)\n",
        "retina_df['level_cat'] = pd.get_dummies(retina_df['level']).values.tolist() # one-hot targets\n",
        "\n",
        "print(retina_df['exists'].sum(), 'images found of', retina_df.shape[0], 'total')\n",
        "\n",
        "#dropna will eliminate na usage rows\n",
        "#retina_df.dropna(inplace = True)\n",
        "\n",
        "print(len(retina_df))\n",
        "print(retina_df['exists'].sum(), 'images found of', retina_df.shape[0], 'total')\n",
        "\n",
        "retina_df = retina_df[retina_df['exists']]\n",
        "\n",
        "print(len(retina_df))\n",
        "\n",
        "retina_df.head(5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bl0jeZEvZcg3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "base_image_dir = os.path.join('..', 'input', 'diabetic-retinopathy-detection')\n",
        "retina_df = pd.read_csv(os.path.join(base_image_dir, 'allLabels.csv'))\n",
        "retina_df['PatientId'] = retina_df['image'].map(lambda x: x.split('_')[0])\n",
        "retina_df['path'] = retina_df['image'].map(lambda x: os.path.join(base_image_dir,\n",
        "                                                         '{}.jpeg'.format(x)))\n",
        "retina_df['exists'] = retina_df['path'].map(os.path.exists)\n",
        "print(retina_df['exists'].sum(), 'images found of', retina_df.shape[0], 'total')\n",
        "retina_df['eye'] = retina_df['image'].map(lambda x: 1 if x.split('_')[-1]=='left' else 0)\n",
        "from keras.utils.np_utils import to_categorical\n",
        "retina_df['level_cat'] = retina_df['level'].map(lambda x: to_categorical(x, 1+retina_df['level'].max()))\n",
        "\n",
        "retina_df.dropna(inplace = True)\n",
        "retina_df = retina_df[retina_df['exists']]\n",
        "retina_df.sample(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "688e4340238e013b8459b6f6470993c7de492d83",
        "_cell_guid": "818da6ca-bbff-4ca0-ad57-ef3a145ae863",
        "id": "oKOpVIgUcW_O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Examine the distribution of eye and severity"
      ]
    },
    {
      "metadata": {
        "_uuid": "60a8111c4093ca6f69d27a4499442ba7dd750839",
        "_cell_guid": "5c8bd288-8261-4cbe-a954-e62ac795cc3e",
        "trusted": true,
        "id": "OJPECJXBcW_P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "retina_df[['level', 'eye']].hist(figsize = (10, 5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4df45776bae0b8a1bf9d3eb4eaaebce6e24d726d",
        "_cell_guid": "0ba697ed-85bb-4e9a-9765-4c367db078d1",
        "id": "ZLCPVMT0cW_W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Split Data into Training and Validation"
      ]
    },
    {
      "metadata": {
        "_uuid": "a48b300ca4d37a6e8b39f82e3c172739635e4baa",
        "_cell_guid": "1192c6b3-a940-4fa0-a498-d7e0d400a796",
        "trusted": true,
        "id": "uUzZdWc4cW_X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "rr_df = retina_df[['PatientId', 'level']].drop_duplicates()\n",
        "train_ids, valid_ids = train_test_split(rr_df['PatientId'], \n",
        "                                   test_size = 0.25, \n",
        "                                   random_state = 2018,\n",
        "                                   stratify = rr_df['level'])\n",
        "raw_train_df = retina_df[retina_df['PatientId'].isin(train_ids)]\n",
        "valid_df = retina_df[retina_df['PatientId'].isin(valid_ids)]\n",
        "print('train', raw_train_df.shape[0], 'validation', valid_df.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "26e566d6cec5bd41f9afe392f456ddf7ceb306ea",
        "_cell_guid": "f8060459-da1e-4293-8f61-c7f99de1de9f",
        "id": "sYzUeStGcW_Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Balance the distribution in the training set"
      ]
    },
    {
      "metadata": {
        "_uuid": "ba7befa238b8c11f9672e3539ac58f3da6955bd9",
        "_cell_guid": "7a130199-fbf6-4c60-95f5-0797b2f3eaf1",
        "trusted": true,
        "id": "jQbbA0BqcW_c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df = raw_train_df.groupby(['level', 'eye']).apply(lambda x: x.sample(75, replace = True)\n",
        "                                                      ).reset_index(drop = True)\n",
        "print('New Data Size:', train_df.shape[0], 'Old Size:', raw_train_df.shape[0])\n",
        "train_df[['level', 'eye']].hist(figsize = (10, 5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9529ab766763a9f122786464c24ab1ebe22c6006",
        "_cell_guid": "9954bfda-29bd-4c4d-b526-0a972b3e43e2",
        "trusted": true,
        "id": "Z3qHynwxcW_e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "import numpy as np\n",
        "IMG_SIZE = (512, 512) # slightly smaller than vgg16 normally expects\n",
        "def tf_image_loader(out_size, \n",
        "                      horizontal_flip = True, \n",
        "                      vertical_flip = False, \n",
        "                     random_brightness = True,\n",
        "                     random_contrast = True,\n",
        "                    random_saturation = True,\n",
        "                    random_hue = True,\n",
        "                      color_mode = 'rgb',\n",
        "                       preproc_func = preprocess_input,\n",
        "                       on_batch = False):\n",
        "    def _func(X):\n",
        "        with tf.name_scope('image_augmentation'):\n",
        "            with tf.name_scope('input'):\n",
        "                X = tf.image.decode_png(tf.read_file(X), channels = 3 if color_mode == 'rgb' else 0)\n",
        "                X = tf.image.resize_images(X, out_size)\n",
        "            with tf.name_scope('augmentation'):\n",
        "                if horizontal_flip:\n",
        "                    X = tf.image.random_flip_left_right(X)\n",
        "                if vertical_flip:\n",
        "                    X = tf.image.random_flip_up_down(X)\n",
        "                if random_brightness:\n",
        "                    X = tf.image.random_brightness(X, max_delta = 0.1)\n",
        "                if random_saturation:\n",
        "                    X = tf.image.random_saturation(X, lower = 0.75, upper = 1.5)\n",
        "                if random_hue:\n",
        "                    X = tf.image.random_hue(X, max_delta = 0.15)\n",
        "                if random_contrast:\n",
        "                    X = tf.image.random_contrast(X, lower = 0.75, upper = 1.5)\n",
        "                return preproc_func(X)\n",
        "    if on_batch: \n",
        "        # we are meant to use it on a batch\n",
        "        def _batch_func(X, y):\n",
        "            return tf.map_fn(_func, X), y\n",
        "        return _batch_func\n",
        "    else:\n",
        "        # we apply it to everything\n",
        "        def _all_func(X, y):\n",
        "            return _func(X), y         \n",
        "        return _all_func\n",
        "    \n",
        "def tf_augmentor(out_size,\n",
        "                intermediate_size = (640, 640),\n",
        "                 intermediate_trans = 'crop',\n",
        "                 batch_size = 16,\n",
        "                   horizontal_flip = True, \n",
        "                  vertical_flip = False, \n",
        "                 random_brightness = True,\n",
        "                 random_contrast = True,\n",
        "                 random_saturation = True,\n",
        "                    random_hue = True,\n",
        "                  color_mode = 'rgb',\n",
        "                   preproc_func = preprocess_input,\n",
        "                   min_crop_percent = 0.001,\n",
        "                   max_crop_percent = 0.005,\n",
        "                   crop_probability = 0.5,\n",
        "                   rotation_range = 10):\n",
        "    \n",
        "    load_ops = tf_image_loader(out_size = intermediate_size, \n",
        "                               horizontal_flip=horizontal_flip, \n",
        "                               vertical_flip=vertical_flip, \n",
        "                               random_brightness = random_brightness,\n",
        "                               random_contrast = random_contrast,\n",
        "                               random_saturation = random_saturation,\n",
        "                               random_hue = random_hue,\n",
        "                               color_mode = color_mode,\n",
        "                               preproc_func = preproc_func,\n",
        "                               on_batch=False)\n",
        "    def batch_ops(X, y):\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        with tf.name_scope('transformation'):\n",
        "            # code borrowed from https://becominghuman.ai/data-augmentation-on-gpu-in-tensorflow-13d14ecf2b19\n",
        "            # The list of affine transformations that our image will go under.\n",
        "            # Every element is Nx8 tensor, where N is a batch size.\n",
        "            transforms = []\n",
        "            identity = tf.constant([1, 0, 0, 0, 1, 0, 0, 0], dtype=tf.float32)\n",
        "            if rotation_range > 0:\n",
        "                angle_rad = rotation_range / 180 * np.pi\n",
        "                angles = tf.random_uniform([batch_size], -angle_rad, angle_rad)\n",
        "                transforms += [tf.contrib.image.angles_to_projective_transforms(angles, intermediate_size[0], intermediate_size[1])]\n",
        "\n",
        "            if crop_probability > 0:\n",
        "                crop_pct = tf.random_uniform([batch_size], min_crop_percent, max_crop_percent)\n",
        "                left = tf.random_uniform([batch_size], 0, intermediate_size[0] * (1.0 - crop_pct))\n",
        "                top = tf.random_uniform([batch_size], 0, intermediate_size[1] * (1.0 - crop_pct))\n",
        "                crop_transform = tf.stack([\n",
        "                      crop_pct,\n",
        "                      tf.zeros([batch_size]), top,\n",
        "                      tf.zeros([batch_size]), crop_pct, left,\n",
        "                      tf.zeros([batch_size]),\n",
        "                      tf.zeros([batch_size])\n",
        "                  ], 1)\n",
        "                coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), crop_probability)\n",
        "                transforms += [tf.where(coin, crop_transform, tf.tile(tf.expand_dims(identity, 0), [batch_size, 1]))]\n",
        "            if len(transforms)>0:\n",
        "                X = tf.contrib.image.transform(X,\n",
        "                      tf.contrib.image.compose_transforms(*transforms),\n",
        "                      interpolation='BILINEAR') # or 'NEAREST'\n",
        "            if intermediate_trans=='scale':\n",
        "                X = tf.image.resize_images(X, out_size)\n",
        "            elif intermediate_trans=='crop':\n",
        "                X = tf.image.resize_image_with_crop_or_pad(X, out_size[0], out_size[1])\n",
        "            else:\n",
        "                raise ValueError('Invalid Operation {}'.format(intermediate_trans))\n",
        "            return X, y\n",
        "    def _create_pipeline(in_ds):\n",
        "        batch_ds = in_ds.map(load_ops, num_parallel_calls=4).batch(batch_size)\n",
        "        return batch_ds.map(batch_ops)\n",
        "    return _create_pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "07851e798db3d89ba13db7d4b56ab2b759221464",
        "_cell_guid": "b5767f42-da63-4737-8f50-749c1a25aa84",
        "trusted": true,
        "id": "DehYQIjCcW_g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def flow_from_dataframe(idg, \n",
        "                        in_df, \n",
        "                        path_col,\n",
        "                        y_col, \n",
        "                        shuffle = True, \n",
        "                        color_mode = 'rgb'):\n",
        "    files_ds = tf.data.Dataset.from_tensor_slices((in_df[path_col].values, \n",
        "                                                   np.stack(in_df[y_col].values,0)))\n",
        "    in_len = in_df[path_col].values.shape[0]\n",
        "    while True:\n",
        "        if shuffle:\n",
        "            files_ds = files_ds.shuffle(in_len) # shuffle the whole dataset\n",
        "        \n",
        "        next_batch = idg(files_ds).repeat().make_one_shot_iterator().get_next()\n",
        "        for i in range(max(in_len//32,1)):\n",
        "            # NOTE: if we loop here it is 'thread-safe-ish' if we loop on the outside it is completely unsafe\n",
        "            yield K.get_session().run(next_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1848f5048a9e00668c3778a85deea97f980e4f1c",
        "_cell_guid": "810bd229-fec9-43c4-b3bd-afd62e3e9552",
        "trusted": true,
        "id": "LDvY35-5cW_j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 48\n",
        "core_idg = tf_augmentor(out_size = IMG_SIZE, \n",
        "                        color_mode = 'rgb', \n",
        "                        vertical_flip = True,\n",
        "                        crop_probability=0.0, # crop doesn't work yet\n",
        "                        batch_size = batch_size) \n",
        "valid_idg = tf_augmentor(out_size = IMG_SIZE, color_mode = 'rgb', \n",
        "                         crop_probability=0.0, \n",
        "                         horizontal_flip = False, \n",
        "                         vertical_flip = False, \n",
        "                         random_brightness = False,\n",
        "                         random_contrast = False,\n",
        "                         random_saturation = False,\n",
        "                         random_hue = False,\n",
        "                         rotation_range = 0,\n",
        "                        batch_size = batch_size)\n",
        "\n",
        "train_gen = flow_from_dataframe(core_idg, train_df, \n",
        "                             path_col = 'path',\n",
        "                            y_col = 'level_cat')\n",
        "\n",
        "valid_gen = flow_from_dataframe(valid_idg, valid_df, \n",
        "                             path_col = 'path',\n",
        "                            y_col = 'level_cat') # we can use much larger batches for evaluation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2ad184b936d5cebae91a265a247d8e0e25920566",
        "id": "DNnH_z64cW_l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Validation Set\n",
        "We do not perform augmentation at all on these images"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6810407e25b887dd8b352f1e46fb3faceaa58ab7",
        "id": "3_EijzFpcW_n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t_x, t_y = next(valid_gen)\n",
        "fig, m_axs = plt.subplots(2, 4, figsize = (16, 8))\n",
        "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
        "    c_ax.imshow(np.clip(c_x*127+127, 0, 255).astype(np.uint8))\n",
        "    c_ax.set_title('Severity {}'.format(np.argmax(c_y, -1)))\n",
        "    c_ax.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "34ce892a19c9734511e2da1d0f2552b361dc826d",
        "id": "tQnzzyBicW_p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Set\n",
        "These are augmented and a real mess"
      ]
    },
    {
      "metadata": {
        "_uuid": "8190b4ad60d49fa65af074dd138a19cb8787e983",
        "scrolled": true,
        "_cell_guid": "2d62234f-aeb0-4eba-8a38-d713d819abf6",
        "trusted": true,
        "id": "O89yoc9acW_p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t_x, t_y = next(train_gen)\n",
        "fig, m_axs = plt.subplots(2, 4, figsize = (16, 8))\n",
        "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
        "    c_ax.imshow(np.clip(c_x*127+127, 0, 255).astype(np.uint8))\n",
        "    c_ax.set_title('Severity {}'.format(np.argmax(c_y, -1)))\n",
        "    c_ax.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "55d665e1e8a8d83b9db005a66a965f8a90c62da1",
        "_cell_guid": "da22790a-672c-474e-b118-9eef15b53160",
        "id": "TFbJIIancW_s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Attention Model\n",
        "The basic idea is that a Global Average Pooling is too simplistic since some of the regions are more relevant than others. So we build an attention mechanism to turn pixels in the GAP on an off before the pooling and then rescale (Lambda layer) the results based on the number of pixels. The model could be seen as a sort of 'global weighted average' pooling. There is probably something published about it and it is very similar to the kind of attention models used in NLP.\n",
        "It is largely based on the insight that the winning solution annotated and trained a UNET model to segmenting the hand and transforming it. This seems very tedious if we could just learn attention."
      ]
    },
    {
      "metadata": {
        "_uuid": "1f0dfaccda346d7bc4758e7329d61028d254a8d6",
        "_cell_guid": "eeb36110-0cde-4450-a43c-b8f707adb235",
        "trusted": true,
        "id": "xTZ4znf6cW_s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16 as PTModel\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2 as PTModel\n",
        "from keras.applications.inception_v3 import InceptionV3 as PTModel\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda\n",
        "from keras.models import Model\n",
        "in_lay = Input(t_x.shape[1:])\n",
        "base_pretrained_model = PTModel(input_shape =  t_x.shape[1:], include_top = False, weights = 'imagenet')\n",
        "base_pretrained_model.trainable = False\n",
        "pt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\n",
        "pt_features = base_pretrained_model(in_lay)\n",
        "from keras.layers import BatchNormalization\n",
        "bn_features = BatchNormalization()(pt_features)\n",
        "\n",
        "# here we do an attention mechanism to turn pixels in the GAP on an off\n",
        "\n",
        "attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(Dropout(0.5)(bn_features))\n",
        "attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n",
        "attn_layer = Conv2D(8, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n",
        "attn_layer = Conv2D(1, \n",
        "                    kernel_size = (1,1), \n",
        "                    padding = 'valid', \n",
        "                    activation = 'sigmoid')(attn_layer)\n",
        "# fan it out to all of the channels\n",
        "up_c2_w = np.ones((1, 1, 1, pt_depth))\n",
        "up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n",
        "               activation = 'linear', use_bias = False, weights = [up_c2_w])\n",
        "up_c2.trainable = False\n",
        "attn_layer = up_c2(attn_layer)\n",
        "\n",
        "mask_features = multiply([attn_layer, bn_features])\n",
        "gap_features = GlobalAveragePooling2D()(mask_features)\n",
        "gap_mask = GlobalAveragePooling2D()(attn_layer)\n",
        "# to account for missing values from the attention model\n",
        "gap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\n",
        "gap_dr = Dropout(0.25)(gap)\n",
        "dr_steps = Dropout(0.25)(Dense(128, activation = 'relu')(gap_dr))\n",
        "out_layer = Dense(t_y.shape[-1], activation = 'softmax')(dr_steps)\n",
        "retina_model = Model(inputs = [in_lay], outputs = [out_layer])\n",
        "from keras.metrics import top_k_categorical_accuracy\n",
        "def top_2_accuracy(in_gt, in_pred):\n",
        "    return top_k_categorical_accuracy(in_gt, in_pred, k=2)\n",
        "\n",
        "retina_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n",
        "                           metrics = ['categorical_accuracy', top_2_accuracy])\n",
        "#retina_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n",
        "#                           metrics = {'categorical_accuracy': top_2_accuracy})\n",
        "retina_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "48b9764e16fb5af52aed35c82bae6299e67d5bc7",
        "_cell_guid": "17803ae1-bed8-41a4-9a2c-e66287a24830",
        "trusted": true,
        "id": "A3Cqyq0kcW_v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
        "#weight_path=\"{}_weights.best.hdf5\".format('retina')\n",
        "weight_path = join(MASTER_PATH, 'tmp', \"retina_weights.best.hdf5\".format('retina'))\n",
        "#weight_path = '/content/_gdrive/My Drive/_colab/models/saved_models'\n",
        "\n",
        "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
        "                             save_best_only=True, mode='min', save_weights_only = False)\n",
        "\n",
        "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n",
        "early = EarlyStopping(monitor=\"val_loss\", \n",
        "                      mode=\"min\", \n",
        "                      patience=6) # probably needs to be more patient, but kaggle time is limited\n",
        "callbacks_list = [checkpoint, early, reduceLROnPlat]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "78dfa383c51777377c1f81e42017cbcca5f5736f",
        "_cell_guid": "84f7cdec-ca00-460c-9991-55b1f7f02f20",
        "trusted": true,
        "id": "PqqkdhovcW_x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf ~/.keras # clean up before starting training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OJyxeZ_9okd8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "def elapsed_time(process):\n",
        "  start = timer()\n",
        "  process\n",
        "  end = timer()\n",
        "  print('elapsed time: {} hours'.format((end - start)/3600)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WuRdbtlsORBJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "elapsed_time(retina_model.fit_generator(train_gen, \n",
        "                           steps_per_epoch = train_df.shape[0]//batch_size,\n",
        "                           validation_data = valid_gen, \n",
        "                           validation_steps = valid_df.shape[0]//batch_size,\n",
        "                              epochs = 25, \n",
        "                              callbacks = callbacks_list,\n",
        "                             workers = 0, # tf-generators are not thread-safe\n",
        "                             use_multiprocessing=False, \n",
        "                             max_queue_size = 0\n",
        "                            ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zjvlgYQncW_6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "retina_model.fit_generator(train_gen, \n",
        "                           steps_per_epoch = train_df.shape[0]//batch_size,\n",
        "                           validation_data = valid_gen, \n",
        "                           validation_steps = valid_df.shape[0]//batch_size,\n",
        "                              epochs = 25, \n",
        "                              callbacks = callbacks_list,\n",
        "                             workers = 0, # tf-generators are not thread-safe\n",
        "                             use_multiprocessing=False, \n",
        "                             max_queue_size = 0\n",
        "                            )"
      ]
    },
    {
      "metadata": {
        "_uuid": "3a90f05dd206cd76c72d8c6278ebb93da41ee45f",
        "_cell_guid": "4d0c45b0-bb23-48d2-83eb-bc3990043e26",
        "trusted": true,
        "id": "wVf79P4kcW_-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load and save the best version of the model\n",
        "#skip this because implemented in checkpoints instead\n",
        "retina_model.load_weights(weight_path)\n",
        "retina_model.save(join(MASTER_PATH, 'tmp', 'full_retina_model.h5'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i6Nh6qF3nWPw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install -U -q PyDrive\n",
        "os.chdir(MASTER_PATH)\n",
        "os.getcwd()\n",
        "import Google_Colaboratory_backup as gcb\n",
        "gcb.backup_pydrive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z2fqu_wughDd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Before session crashes because gpu memory naers full"
      ]
    },
    {
      "metadata": {
        "id": "cR4TGCnaA3eP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# release gpu memory by killing processes\n",
        "%%bash\n",
        "! apt-get install psmisc -y\n",
        "sudo fuser -v /dev/nvidia*\n",
        "#kill -9 PID"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J6SlcPhrCriV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# If session crashes"
      ]
    },
    {
      "metadata": {
        "id": "_ra_U4ZAmRVs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "import os\n",
        "from os.path import join\n",
        "from google.colab import drive\n",
        "\n",
        "ROOT = \"/content/_gdrive\"\n",
        "drive.mount(ROOT, force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HMyfpPwPMdwr",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reload model\n",
        "# first upload the modified script\n",
        "# then move files over to the master directory from /content  \n",
        "%%bash\n",
        "cd \"/content\"\n",
        "mv reload_model.py '/content/master-inceptionv3-for-retinopathy/'\n",
        "mv Google_Colaboratory_backup.py '/content/master-inceptionv3-for-retinopathy/'\n",
        "\n",
        "# now reload\n",
        "cd 'content/master-inceptionv3-for-retinopathy'\n",
        "python reload_model.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3mbae0MfqWj5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from reload_model import *\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt # showing and rendering figures\n",
        "# io related\n",
        "from skimage.io import imread\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "########\n",
        "then need to load the saved model as well"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "koODYrixAAHJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "os.chdir(MASTER_PATH)\n",
        "os.getcwd()\n",
        "import Google_Colaboratory_backup as gcb\n",
        "gcb.restore_pydrive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CtkJwuU8I7j7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! ls -la '/content/_gdrive/My Drive/_colab/backup'\n",
        "! ls -la '/content/master-inceptionv3-for-retinopathy/tmp'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_a74GG4O4dKb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# redefine paths in case session has just crashed\n",
        "\n",
        "import os\n",
        "from os.path import join\n",
        "master_folder = 'master-inceptionv3-for-retinopathy'\n",
        "MASTER_PATH = join('/content', master_folder)\n",
        "\n",
        "# reload model\n",
        "import keras\n",
        "#from keras.models import Model\n",
        "\n",
        "retina_model = keras.models.load_model(\n",
        "    join(MASTER_PATH, 'tmp', 'full_retina_model.h5'), \n",
        "    compile=False)\n",
        "#retina_model = keras.models.load_model(\n",
        "#    '/content/master-inceptionv3-for-retinopathy/tmp/full_retina_model.h5',\n",
        "#    compile=False)\n",
        "\n",
        "retina_model.summary()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y7i3zrvlXpfX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load and save the best version of the model\n",
        "# retina_model.load_weights(weight_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YnU8sHsbqyfL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from reload_model import *\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2b74f4ab850c6e82549d732b6f0524724b95b53c",
        "_cell_guid": "f37dd4d8-ecd6-487a-90d8-74fe14a9a318",
        "trusted": true,
        "id": "n7kuY49QcXAA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### create one fixed dataset for evaluating\n",
        "from tqdm import tqdm_notebook\n",
        "# fresh valid gen\n",
        "valid_gen = flow_from_dataframe(valid_idg, valid_df, \n",
        "                             path_col = 'path',\n",
        "                            y_col = 'level_cat') \n",
        "\n",
        "t_x, t_y = next(valid_gen)\n",
        "\n",
        "vbatch_count = (valid_df.shape[0]//batch_size-1)\n",
        "out_size = vbatch_count*batch_size\n",
        "test_X = np.zeros((out_size,)+t_x.shape[1:], dtype = np.float32)\n",
        "test_Y = np.zeros((out_size,)+t_y.shape[1:], dtype = np.float32)\n",
        "for i, (c_x, c_y) in zip(tqdm_notebook(range(vbatch_count)), \n",
        "                         valid_gen):\n",
        "    j = i*batch_size\n",
        "    test_X[j:(j+c_x.shape[0])] = c_x\n",
        "    test_Y[j:(j+c_x.shape[0])] = c_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "crdUwMqcMLC9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cca170eb40bc591f89748ede8aa35de4308faaaf",
        "_cell_guid": "11f33f0a-61eb-488a-b7ea-4bc9d15ba8f9",
        "id": "IBF8HQIKcXAC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Show Attention\n",
        "Did our attention model learn anything useful?"
      ]
    },
    {
      "metadata": {
        "_uuid": "ad5b085d351e79b950bf0c2ddc476799d5b0692f",
        "_cell_guid": "e41a063f-35c9-410f-be63-f66b63ff9683",
        "trusted": true,
        "id": "9eeKsqPdcXAD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get the attention layer since it is the only one with a single output dim\n",
        "for attn_layer in retina_model.layers:\n",
        "    c_shape = attn_layer.get_output_shape_at(0)\n",
        "    if len(c_shape)==4:\n",
        "        if c_shape[-1]==1:\n",
        "            print(attn_layer)\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "00850972ae4298f49ed1838b3fc49c2d8fb07547",
        "_cell_guid": "340eef36-f5b2-4b15-a59f-440061a427eb",
        "trusted": true,
        "id": "bmH7vUGRcXAG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "rand_idx = np.random.choice(range(len(test_X)), size = 6)\n",
        "attn_func = K.function(inputs = [retina_model.get_input_at(0), K.learning_phase()],\n",
        "           outputs = [attn_layer.get_output_at(0)]\n",
        "          )\n",
        "fig, m_axs = plt.subplots(len(rand_idx), 2, figsize = (8, 4*len(rand_idx)))\n",
        "[c_ax.axis('off') for c_ax in m_axs.flatten()]\n",
        "for c_idx, (img_ax, attn_ax) in zip(rand_idx, m_axs):\n",
        "    cur_img = test_X[c_idx:(c_idx+1)]\n",
        "    attn_img = attn_func([cur_img, 0])[0]\n",
        "    img_ax.imshow(np.clip(cur_img[0,:,:,:]*127+127, 0, 255).astype(np.uint8))\n",
        "    attn_ax.imshow(attn_img[0, :, :, 0]/attn_img[0, :, :, 0].max(), cmap = 'viridis', \n",
        "                   vmin = 0, vmax = 1, \n",
        "                   interpolation = 'lanczos')\n",
        "    real_cat = np.argmax(test_Y[c_idx, :])\n",
        "    img_ax.set_title('Eye Image\\nCat:%2d' % (real_cat))\n",
        "    pred_cat = retina_model.predict(cur_img)\n",
        "    attn_ax.set_title('Attention Map\\nPred:%2.2f%%' % (100*pred_cat[0,real_cat]))\n",
        "fig.savefig(\n",
        "    join(MASTER_PATH, 'tmp', 'attention_map.png'), \n",
        "    dpi = 300)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "244bac80d1ea2074e47932e367996e32cbab6a3d",
        "_cell_guid": "24796de7-b1e9-4b3b-bcc6-d997aa3e6d16",
        "id": "dmMERra1cXAJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluate the results\n",
        "Here we evaluate the results by loading the best version of the model and seeing how the predictions look on the results. We then visualize spec"
      ]
    },
    {
      "metadata": {
        "_uuid": "b421b6183b1919a7414482f0b1ac611079e45174",
        "_cell_guid": "d0edaf00-4b7c-4f65-af0b-e5a03b9b8428",
        "trusted": true,
        "id": "2EA69OeDcXAK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "pred_Y = retina_model.predict(test_X, batch_size = 32, verbose = True)\n",
        "pred_Y_cat = np.argmax(pred_Y, -1)\n",
        "test_Y_cat = np.argmax(test_Y, -1)\n",
        "print('Accuracy on Test Data: %2.2f%%' % (accuracy_score(test_Y_cat, pred_Y_cat)))\n",
        "print(classification_report(test_Y_cat, pred_Y_cat))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "10162e055ca7cd52878a289bab377231787ab732",
        "_cell_guid": "15189df2-3fed-495e-9661-97bb2b712dfd",
        "trusted": true,
        "id": "U3wGeMxVcXAP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "sns.heatmap(confusion_matrix(test_Y_cat, pred_Y_cat), \n",
        "            annot=True, fmt=\"d\", cbar = False, cmap = plt.cm.Blues, vmax = test_X.shape[0]//16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "12dfe39ea80194062068589699953c6645e285d6",
        "_cell_guid": "70827da6-bf91-4b65-80e9-bf1e6b885db3",
        "id": "r3V3_DLBcXAR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ROC Curve for healthy vs sick\n",
        "Here we make an ROC curve for healthy (```severity == 0```) and sick (```severity>0```) to see how well the model works at just identifying the disease"
      ]
    },
    {
      "metadata": {
        "_uuid": "2b2aaee6c83043f721b0c9ed5bc4229eb7165200",
        "_cell_guid": "829475ab-7db2-4421-b9ad-1a51971fd459",
        "trusted": true,
        "id": "JuoHmpFqcXAS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "sick_vec = test_Y_cat>0\n",
        "sick_score = np.sum(pred_Y[:,1:],1)\n",
        "fpr, tpr, _ = roc_curve(sick_vec, sick_score)\n",
        "fig, ax1 = plt.subplots(1,1, figsize = (6, 6), dpi = 150)\n",
        "ax1.plot(fpr, tpr, 'b.-', label = 'Model Prediction (AUC: %2.2f)' % roc_auc_score(sick_vec, sick_score))\n",
        "ax1.plot(fpr, fpr, 'g-', label = 'Random Guessing')\n",
        "ax1.legend()\n",
        "ax1.set_xlabel('False Positive Rate')\n",
        "ax1.set_ylabel('True Positive Rate');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ba87d0e7c3a77181487b99ca64d13de2aa8a21ee",
        "scrolled": false,
        "_cell_guid": "c34f049f-b032-45bf-9d5e-a756ecc46a82",
        "trusted": true,
        "id": "GZsqtoG0cXAU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, m_axs = plt.subplots(2, 4, figsize = (32, 20))\n",
        "for (idx, c_ax) in enumerate(m_axs.flatten()):\n",
        "    c_ax.imshow(np.clip(test_X[idx]*127+127,0 , 255).astype(np.uint8), cmap = 'bone')\n",
        "    c_ax.set_title('Actual Severity: {}\\n{}'.format(test_Y_cat[idx], \n",
        "                                                           '\\n'.join(['Predicted %02d (%04.1f%%): %s' % (k, 100*v, '*'*int(10*v)) for k, v in sorted(enumerate(pred_Y[idx]), key = lambda x: -1*x[1])])), loc='left')\n",
        "    c_ax.axis('off')\n",
        "fig.savefig(\n",
        "    join(MASTER_PATH, 'tmp', 'trained_img_predictions.png'), \n",
        "    dpi = 300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eb6752295030ba512263433f8383711e4ca1c14c",
        "_cell_guid": "f2e189dc-f80a-4b16-bb1d-5c05a155a80b",
        "trusted": true,
        "id": "Wy7aAO5bcXAX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QnO-SmAZopR-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download Kaggle Data"
      ]
    },
    {
      "metadata": {
        "id": "TvowMUgln9zh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "user = 'pichetzh'\n",
        "key = '489ef19bcb40e9d0ceb8e91a19481ecb'\n",
        "#{\"username\":\"pichetzh\",\"key\":\"489ef19bcb40e9d0ceb8e91a19481ecb\"}\n",
        "\n",
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "if '.kaggle' not in os.listdir('/root'):\n",
        "    !mkdir ~/.kaggle\n",
        "!touch /root/.kaggle/kaggle.json\n",
        "!chmod 666 /root/.kaggle/kaggle.json\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
        "    f.write('{\"username\":\"%s\",\"key\":\"%s\"}' % (user, key))\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RCKiIx-CowHs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pwd\n",
        "#mkdir -p '/content/_data/eyepacs'\n",
        "cd '/content/_github/data/eyepacs'\n",
        "#kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "kaggle competitions download -c diabetic-retinopathy-detection\n",
        "#kaggle datasets download -d pichetzh/eyepacs-labels\n",
        "#kaggle datasets download -d pichetzh/eyepacs-subset-train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ujw1PG0qFoI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kaggle datasets list -m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJNcSTjZPt-y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kaggle datasets list -h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BUNVBQ2GVDxD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pre-processing images"
      ]
    },
    {
      "metadata": {
        "id": "9aWZ8d-pVOVc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# copy file from google drive\n",
        "%%bash\n",
        "#source = '/content/_gdrive/My Drive/_colab/dataset/messidor.zip'\n",
        "#dest = '/content/_github/data/messidor'\n",
        "\n",
        "cp '/content/_gdrive/My Drive/_colab/dataset/messidor.zip' '/content/_github/data'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4uMWdDxVXUIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# unzip file\n",
        "%%bash\n",
        "cd '/content/_github/data/messidor'\n",
        "ls\n",
        "unzip messidor.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VH4HE88YW10W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd '/content/_github'\n",
        "pwd\n",
        "./eyepacs.sh --only_gradable\n",
        "#./messidor.sh --only_gradable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vwTJMA_KyKpa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "starts at 3:05am"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vjCOnsiDzz7t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# remove zip file from eyepacs\n",
        "%%bash\n",
        "cd /content/_github/data/eyepacs\n",
        "rm -rf *.zip.*\n",
        "\n",
        "# make zip file and transfer to google drive\n",
        "cd /content/_github/data/\n",
        "zip -r eyepacs eyepacs\n",
        "mv eyepacs.zip /content/_gdrive/'My Drive'/_colab/data/diabetic_retinopathy/jama16/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYfErVQaACKE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# send email notification\n",
        "pip install yagmail\n",
        "cd /content\n",
        "python send_yagmail.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4qn3n4fQfkYH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd '/content/_github/data/'\n",
        "\n",
        "#cp messidor.zip /content/_gdrive/'My Drive'/_colab/dataset/\n",
        "cp messidor.zip /content/_data\n",
        "#rm messidor.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cIZYrtOL3pf4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "mkdir -p /content/_data/messidor/bin2\n",
        "cd /content/_github/data/messidor\n",
        "pwd\n",
        "cp -R *.xls /content/_data/messidor/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mIes-KZNeT-6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd '/content/_data/'\n",
        "zip -r messidor messidor\n",
        "\n",
        "#mv messidor.zip /content/_gdrive/'My Drive'/_colab/dataset/\n",
        "mv messidor.zip /content/_gdrive/'My Drive'/_colab/data/diabetic_retinopathy/jama16/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pEQ91RyUYUvf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd /content/_github\n",
        "pwd\n",
        "python train.py -h "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z-PwslHgkTM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}